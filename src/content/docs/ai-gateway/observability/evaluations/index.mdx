---
title: Evaluations
pcx_content_type: navigation
order: 1
---

# Overview

Understanding your application's performance is essential for optimizing and improving it. Developers often have different priorities, and finding the optimal solution involves balancing key factors such as cost, latency, and accuracy. For some, low-latency responses are critical, while others may prioritize accuracy or cost-efficiency.

AI Gateway's Evaluations provides the data needed to make informed decisions on how to optimize your AI application. Whether it's adjusting the model, provider, or prompt, this feature delivers insights into key metrics around performance, speed, and cost. It empowers developers to better understand their application's behavior, ensuring improved accuracy, reliability, and customer satisfaction.

Datasets are collections of logs stored for analysis that can be used for an evaluation. You can create datasets by applying filters in the Logs or Evaluations tab, which helps narrow down specific logs for evaluation.

Our first step toward comprehensive AI evaluations starts with human-in-the-loop feedback (currently in open beta). We will continue to build and expand AI Gateway with additional evaluators.

[Learn how to set up an evaluation](/set-up-evaluations/) including creating datasets, selecting evaluators, and running the evaluation process.
