model:
  id: "906a57fd-b018-4d6c-a43e-a296d4cc5839"
  source: 1
  name: "@cf/meta/llama-3.2-1b-instruct"
  description: "The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks."
  task:
    id: "c329a1f9-323d-4e91-b2aa-582dd4188d34"
    name: "Text Generation"
    description: "Family of generative text models, such as large language models (LLM), that can be adapted for a variety of natural language tasks."
  tags: []
  properties:
    - property_id: "beta"
      value: "true"
    - property_id: "terms"
      value: "https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE"
task_type: "text-generation"
model_display_name: "llama-3.2-1b-instruct"
layout: "model"
weight: 0
title: "llama-3.2-1b-instruct"
json_schema:
  input: "{\n  \"type\": \"object\",\n  \"oneOf\": [\n    {\n      \"title\": \"Prompt\",\n      \"properties\": {\n        \"prompt\": {\n          \"type\": \"string\",\n          \"minLength\": 1,\n          \"maxLength\": 131072,\n          \"description\": \"The input text prompt for the model to generate a response.\"\n        },\n        \"raw\": {\n          \"type\": \"boolean\",\n          \"default\": false,\n          \"description\": \"If true, a chat template is not applied and you must adhere to the specific model's expected formatting.\"\n        },\n        \"stream\": {\n          \"type\": \"boolean\",\n          \"default\": false,\n          \"description\": \"If true, the response will be streamed back incrementally using SSE, Server Sent Events.\"\n        },\n        \"max_tokens\": {\n          \"type\": \"integer\",\n          \"default\": 256,\n          \"description\": \"The maximum number of tokens to generate in the response.\"\n        },\n        \"temperature\": {\n          \"type\": \"number\",\n          \"default\": 0.6,\n          \"minimum\": 0,\n          \"maximum\": 5,\n          \"description\": \"Controls the randomness of the output; higher values produce more random results.\"\n        },\n        \"top_p\": {\n          \"type\": \"number\",\n          \"minimum\": 0,\n          \"maximum\": 2,\n          \"description\": \"Adjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.\"\n        },\n        \"top_k\": {\n          \"type\": \"integer\",\n          \"minimum\": 1,\n          \"maximum\": 50,\n          \"description\": \"Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.\"\n        },\n        \"seed\": {\n          \"type\": \"integer\",\n          \"minimum\": 1,\n          \"maximum\": 9999999999,\n          \"description\": \"Random seed for reproducibility of the generation.\"\n        },\n        \"repetition_penalty\": {\n          \"type\": \"number\",\n          \"minimum\": 0,\n          \"maximum\": 2,\n          \"description\": \"Penalty for repeated tokens; higher values discourage repetition.\"\n        },\n        \"frequency_penalty\": {\n          \"type\": \"number\",\n          \"minimum\": 0,\n          \"maximum\": 2,\n          \"description\": \"Decreases the likelihood of the model repeating the same lines verbatim.\"\n        },\n        \"presence_penalty\": {\n          \"type\": \"number\",\n          \"minimum\": 0,\n          \"maximum\": 2,\n          \"description\": \"Increases the likelihood of the model introducing new topics.\"\n        }\n      },\n      \"required\": [\n        \"prompt\"\n      ]\n    },\n    {\n      \"title\": \"Messages\",\n      \"properties\": {\n        \"messages\": {\n          \"type\": \"array\",\n          \"description\": \"An array of message objects representing the conversation history.\",\n          \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"role\": {\n                \"type\": \"string\",\n                \"description\": \"The role of the message sender (e.g., 'user', 'assistant', 'system', 'tool').\"\n              },\n              \"content\": {\n                \"oneOf\": [\n                  {\n                    \"type\": \"string\",\n                    \"maxLength\": 131072,\n                    \"description\": \"The content of the message as a string.\"\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"items\": {\n                      \"type\": \"object\",\n                      \"properties\": {\n                        \"type\": {\n                          \"type\": \"string\",\n                          \"enum\": [\n                            \"text\",\n                            \"image_url\"\n                          ],\n                          \"description\": \"The type of content (e.g. 'text', 'image_url').\"\n                        },\n                        \"text\": {\n                          \"type\": \"string\",\n                          \"description\": \"The text content of the message object.\"\n                        },\n                        \"image_url\": {\n                          \"type\": \"object\",\n                          \"properties\": {\n                            \"url\": {\n                              \"type\": \"string\",\n                              \"pattern\": \"^data:[-\\\\w.]+/[-\\\\w.]+;base64,\",\n                              \"description\": \"A base64 data URL containing the encoded image to be used. Should be in the format data:[media-type];base64,.\"\n                            }\n                          }\n                        }\n                      }\n                    }\n                  }\n                ]\n              }\n            },\n            \"required\": [\n              \"role\",\n              \"content\"\n            ]\n          }\n        },\n        \"functions\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"name\": {\n                \"type\": \"string\"\n              },\n              \"code\": {\n                \"type\": \"string\"\n              }\n            },\n            \"required\": [\n              \"name\",\n              \"code\"\n            ]\n          }\n        },\n        \"stream\": {\n          \"type\": \"boolean\",\n          \"default\": false,\n          \"description\": \"If true, the response will be streamed back incrementally.\"\n        },\n        \"max_tokens\": {\n          \"type\": \"integer\",\n          \"default\": 256,\n          \"description\": \"The maximum number of tokens to generate in the response.\"\n        },\n        \"temperature\": {\n          \"type\": \"number\",\n          \"default\": 0.6,\n          \"minimum\": 0,\n          \"maximum\": 5,\n          \"description\": \"Controls the randomness of the output; higher values produce more random results.\"\n        },\n        \"top_p\": {\n          \"type\": \"number\",\n          \"minimum\": 0,\n          \"maximum\": 2,\n          \"description\": \"Controls the creativity of the AI's responses by adjusting how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.\"\n        },\n        \"top_k\": {\n          \"type\": \"integer\",\n          \"minimum\": 1,\n          \"maximum\": 50,\n          \"description\": \"Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.\"\n        },\n        \"seed\": {\n          \"type\": \"integer\",\n          \"minimum\": 1,\n          \"maximum\": 9999999999,\n          \"description\": \"Random seed for reproducibility of the generation.\"\n        },\n        \"repetition_penalty\": {\n          \"type\": \"number\",\n          \"minimum\": 0,\n          \"maximum\": 2,\n          \"description\": \"Penalty for repeated tokens; higher values discourage repetition.\"\n        },\n        \"frequency_penalty\": {\n          \"type\": \"number\",\n          \"minimum\": 0,\n          \"maximum\": 2,\n          \"description\": \"Decreases the likelihood of the model repeating the same lines verbatim.\"\n        },\n        \"presence_penalty\": {\n          \"type\": \"number\",\n          \"minimum\": 0,\n          \"maximum\": 2,\n          \"description\": \"Increases the likelihood of the model introducing new topics.\"\n        }\n      },\n      \"required\": [\n        \"messages\"\n      ]\n    }\n  ]\n}"
  output: "{\n  \"oneOf\": [\n    {\n      \"type\": \"object\",\n      \"contentType\": \"application/json\",\n      \"properties\": {\n        \"response\": {\n          \"type\": \"string\",\n          \"description\": \"The generated text response from the model\"\n        }\n      }\n    },\n    {\n      \"type\": \"string\",\n      \"contentType\": \"text/event-stream\",\n      \"format\": \"binary\"\n    }\n  ]\n}"
